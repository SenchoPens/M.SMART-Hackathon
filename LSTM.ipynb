{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our basic nlp pipeline\n",
    "    1) Stem\n",
    "    2) Add stop words\n",
    "    3) Tokenize\n",
    "    4) Vectorize (probably just embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/deniska/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_text(sentence, stopwords=[None]):\n",
    "    return \" \".join([stemmer.stem(word.decode(\"utf-8\")) for word in sentence.split(\" \")\n",
    "                   if word.decode(\"utf-8\") not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our data\n",
    "data = pd.read_csv(\"X_train.csv\")\n",
    "\n",
    "#out stemmer\n",
    "stemmer = nltk.stem.snowball.RussianStemmer(ignore_stopwords=True)  \n",
    "\n",
    "#our tokenizer\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "#preprocess data\n",
    "data.comment = data.comment.apply(stem_text)\n",
    "comments = list(data.comment)\n",
    "comments.sort(key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enumerated_text = enumerate(set([0] + tokenizer.tokenize(\" \".join(comments))))\n",
    "word2token = {w : t for t, w in enumerated_text}\n",
    "token2word = {t : w for w, t in word2token.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, maxlen):\n",
    "    sentence = np.asarray([word2token[word] for word in tokenizer.tokenize(sentence)])\n",
    "    return sentence\n",
    "    \n",
    "def onehot(y, n_class=5):\n",
    "    Y = np.zeros((len(y), n_class))\n",
    "    Y[np.arange(len(y)), y] = 1\n",
    "    return Y\n",
    "\n",
    "def sample_batch(batch_size):\n",
    "    i = np.random.randint(0, len(comments) - batch_size)\n",
    "        \n",
    "    sens = sorted([tokenize_sentence(sentence, 0) \n",
    "                   for sentence in comments[i : i + batch_size]], key=len)\n",
    "    maxlen = len(sens[-1])\n",
    "    \n",
    "    return np.array([np.pad(sen, (0, maxlen - len(sen)), \"constant\", constant_values=(0))\n",
    "                    for sen in sens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2token.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS=device=gpu0,floatX=float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 1060 6GB (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS=device=gpu0,floatX=float32\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputSentence = T.matrix(\"Sentancia\", \"int32\")\n",
    "inputClass = T.matrix(\"Rating\", \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = InputLayer((None, None), inputSentence)\n",
    "net = EmbeddingLayer(net, VOCAB_SIZE, 128)\n",
    "\n",
    "net = LSTMLayer(net, 256, grad_clipping=100)\n",
    "net = LSTMLayer(net, 256, grad_clipping=100, only_return_final=True)\n",
    "\n",
    "net = DenseLayer(net, 128)\n",
    "net = DenseLayer(net, 5, nonlinearity=T.nnet.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = get_output(net)\n",
    "loss = lasagne.objectives.categorical_crossentropy(output, inputClass).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.adam(loss, params)\n",
    "train_fun = theano.function([inputSentence, inputClass], loss, updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
