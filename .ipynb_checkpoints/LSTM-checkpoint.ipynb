{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our basic nlp pipeline\n",
    "    1) Stem\n",
    "    2) Add stop words\n",
    "    3) Tokenize\n",
    "    4) Vectorize (probably just embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/deniska/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk \n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_text(sentence, stopwords=[None]):\n",
    "    return \" \".join([stemmer.stem(word.decode(\"utf-8\")) for word in sentence.split(\" \")\n",
    "                   if word.decode(\"utf-8\") not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our data\n",
    "data = pd.read_csv(\"X_train.csv\")\n",
    "\n",
    "#out stemmer\n",
    "stemmer = nltk.stem.snowball.RussianStemmer(ignore_stopwords=True)  \n",
    "\n",
    "#our tokenizer\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "\n",
    "#preprocess data\n",
    "data.comment = data.comment.apply(stem_text)\n",
    "comments = list(data.comment)\n",
    "ratings = list(data.reting)\n",
    "\n",
    "ratings = [r for _, r in sorted(zip(comments, ratings), key=len)]\n",
    "comments = sorted(comments, key=len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "enumerated_text = enumerate(set([0] + tokenizer.tokenize(\" \".join(comments))))\n",
    "word2token = {w : t for t, w in enumerated_text}\n",
    "token2word = {t : w for w, t in word2token.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence, maxlen):\n",
    "    sentence = np.asarray([word2token[word] for word in tokenizer.tokenize(sentence)])\n",
    "    return sentence\n",
    "    \n",
    "def onehot(y, n_class=5):\n",
    "    Y = np.zeros((len(y), n_class))\n",
    "    Y[np.arange(len(y)), y] = 1\n",
    "    return Y\n",
    "\n",
    "def sample_batch(batch_size):\n",
    "    i = np.random.randint(0, len(comments) - batch_size)\n",
    "        \n",
    "    sens = sorted([tokenize_sentence(sentence, 0) \n",
    "                   for sentence in comments[i : i + batch_size]], key=len)\n",
    "    maxlen = len(sens[-1])\n",
    "    \n",
    "    return np.array([np.pad(sen, (0, maxlen - len(sen)), \"constant\", constant_values=(0))\n",
    "                    for sen in sens]), np.array(ratings[i : i + batch_size], dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2token.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%env THEANO_FLAGS=device=gpu0,floatX=float32\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "import lasagne\n",
    "from lasagne.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputSentence = T.matrix(\"Sentancia\", \"int32\")\n",
    "inputClass = T.matrix(\"Rating\", \"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = InputLayer((None, None), inputSentence)\n",
    "net = EmbeddingLayer(net, VOCAB_SIZE, 128)\n",
    "\n",
    "net = LSTMLayer(net, 256, grad_clipping=100)\n",
    "net = LSTMLayer(net, 256, grad_clipping=100, only_return_final=True)\n",
    "\n",
    "net = DenseLayer(net, 128)\n",
    "net = DenseLayer(net, 5, nonlinearity=T.nnet.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = get_output(net)\n",
    "loss = lasagne.objectives.categorical_crossentropy(output, inputClass).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = get_all_params(net, trainable=True)\n",
    "updates = lasagne.updates.adam(loss, params)\n",
    "train_fun = theano.function([inputSentence, inputClass], loss, updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training loop\n",
    "\n",
    "num_epochs = 20000\n",
    "batch_size = 64\n",
    "\n",
    "for ep in range(num_epochs):\n",
    "    print ep\n",
    "    x, y = sample_batch(batch_size)\n",
    "    \n",
    "    train_fun(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
